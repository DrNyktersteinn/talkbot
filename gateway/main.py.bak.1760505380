import os
import json
import base64
from typing import Optional, Dict, Any, List

import httpx
from fastapi import FastAPI, Header, HTTPException, UploadFile, File, Form, Body, Request

# ---------------- Env / Config ----------------

OLLAMA_URL  = os.getenv("OLLAMA_URL", "http://127.0.0.1:11434")
CHAT_MODEL  = os.getenv("CHAT_MODEL",  "mistral:latest")
VISION_MODEL= os.getenv("VISION_MODEL","moondream2:latest")

DATA_DIR    = os.getenv("TALKBOT_DATA_DIR", os.path.expanduser("~/talkbot/runtime"))
API_KEYS_FN = os.getenv("API_KEYS_FILE", os.path.join(DATA_DIR, "api_keys.txt"))

api = FastAPI(title="TalkBot Gateway", version="1.0.0")

# ---------------- Auth ----------------

def _load_keys() -> List[str]:
    try:
        with open(API_KEYS_FN, "r", encoding="utf-8") as f:
            keys = [ln.strip() for ln in f if ln.strip()]
        return keys
    except Exception:
        return []

def _require_key(authorization: Optional[str]):
    if not authorization or not authorization.startswith("Bearer "):
        raise HTTPException(status_code=401, detail="Missing Authorization header (Bearer <key>)")
    token = authorization.split(" ", 1)[1].strip()
    keys = _load_keys()
    if token not in keys:
        raise HTTPException(status_code=403, detail=f"Invalid API key (loaded={len(keys)} from {API_KEYS_FN})")

# ---------------- Health & Debug ----------------

@api.get("/health")
def health():
    return {"ok": True}

@api.get("/_authdebug")
def authdebug(authorization: Optional[str] = Header(None)):
    tok = ""
    if authorization and authorization.startswith("Bearer "):
        tok = authorization.split(" ", 1)[1].strip()
    return {
        "authorization": authorization,
        "token_len": len(tok),
        "token_prefix": tok[:8],
        "token_suffix": tok[-8:],
        "API_KEYS_FN": API_KEYS_FN,
        "loaded_keys": len(_load_keys()),
    }

# ---------------- Ollama helpers ----------------

async def ollama_generate(model: str, prompt: str, image_b64: Optional[str] = None) -> str:
    """
    Robust Ollama call:
      - Prefer /api/generate
      - On 404/405 or empty content, transparently fall back to /api/chat
    Vision: pass base64 in 'images' for either endpoint.
    """
    async with httpx.AsyncClient(timeout=120.0) as c:
        # ---------- Attempt A: /api/generate ----------
        try:
            if image_b64:
                payload = {
                    "model": model,
                    "prompt": prompt,
                    "images": [image_b64],
                    "stream": False,
                }
            else:
                payload = {"model": model, "prompt": prompt, "stream": False}

            r = await c.post(f"{OLLAMA_URL}/api/generate", json=payload)
            # Some setups return 404/405 here; let fallback handle those.
            if r.status_code == 200:
                j = r.json()
                content = (j.get("response") or "").strip()
                if content:
                    return content
            else:
                # non-200 -> try fallback
                pass
        except Exception:
            # network / parse -> try fallback
            pass

        # ---------- Attempt B: /api/chat ----------
        try:
            if image_b64:
                # Vision via chat: images on user message
                payload = {
                    "model": model,
                    "messages": [{
                        "role": "user",
                        "content": prompt,
                        "images": [image_b64],
                    }],
                    "stream": False,
                }
            else:
                # Text via chat: render simple user message
                payload = {
                    "model": model,
                    "messages": [
                        {"role": "system", "content": "You are a concise helpful assistant."},
                        {"role": "user", "content": prompt},
                    ],
                    "stream": False,
                }

            r = await c.post(f"{OLLAMA_URL}/api/chat", json=payload)
            r.raise_for_status()
            j = r.json()

            # Common shapes:
            #  - {"message":{"role":"assistant","content":"..."}, ...}
            if isinstance(j, dict):
                msg = j.get("message")
                if isinstance(msg, dict):
                    content = (msg.get("content") or "").strip()
                    if content:
                        return content
                # Some adapters mimic OpenAI:
                #  - {"choices":[{"message":{"content":"..."}}], ...}
                ch = j.get("choices")
                if isinstance(ch, list) and ch:
                    content = (((ch[0] or {}).get("message") or {}).get("content") or "").strip()
                    if content:
                        return content
            # last-ditch
            return (j.get("response") or "").strip() if isinstance(j, dict) else ""
        except Exception:
            return ""

async def ollama_chat_as_generate(model: str, messages: List[Dict[str, str]]) -> str:
    prompt = _render_chat_messages_to_prompt(messages)
    return await ollama_generate(model, prompt, image_b64=None)

# ---------------- TTS stub ----------------

def speak(_text: str, _voice_id: str) -> str:
    """
    Server-side audio not required here; GUI does TTS.
    Return empty string to signal 'no server audio'.
    """
    return ""

# ---------------- Chat ----------------



@api.post("/chat")
async def chat_endpoint(payload: Dict[str, Any], authorization: Optional[str] = Header(None)):
    _require_key(authorization)
    try:
        text = str(payload.get("text", "")).strip()
    except Exception:
        text = ""
    voice_id = str(payload.get("voice_id", "en_US-amy-medium"))
    if not text:
        raise HTTPException(status_code=400, detail="Empty text")

    sys_prompt = "You are a concise helpful assistant. Answer clearly."
    try:
        answer = await ollama_chat(CHAT_MODEL, [
            {"role":"system","content":sys_prompt},
            {"role":"user","content":text}
        ])
    except Exception as e:
        # Surface upstream details so you see *why* it failed
        raise HTTPException(status_code=502, detail=f"chat upstream error: {e}")

    # IMPORTANT: no server TTS here; client (pyttsx3) will speak.
    return {"text": answer or "", "audio_url": "", "voice": voice_id}

async def chat_endpoint(payload: Dict[str, Any], authorization: Optional[str] = Header(None)):
    _require_key(authorization)
    text = str(payload.get("text", "")).strip()
    voice_id = str(payload.get("voice_id", "en_US-amy-medium"))
    if not text:
        raise HTTPException(status_code=400, detail="Empty text")
    # Mistral chat
    sys_prompt = "You are a concise helpful assistant. Answer clearly."
    assistant = await ollama_chat(CHAT_MODEL, [
        {"role": "system", "content": sys_prompt},
        {"role": "user", "content": text}
    ])
    # IMPORTANT: do NOT try server-side TTS here; return empty audio_url so the client speaks locally.
    audio_url = ""
    return {"text": assistant, "audio_url": audio_url, "voice": voice_id}

async def chat_endpoint(payload: Dict[str, Any], authorization: Optional[str] = Header(None)):
    _require_key(authorization)
    text = str(payload.get("text", "")).strip()
    voice_id = str(payload.get("voice_id", "en_US-amy-medium"))
    if not text:
        raise HTTPException(status_code=400, detail="Empty text")
    # System prompt kept minimal; use generate-based chat
    sys_prompt = "You are a concise, helpful assistant. Answer clearly."
    messages = [
        {"role": "system", "content": sys_prompt},
        {"role": "user", "content": text},
    ]
    assistant = await ollama_chat_as_generate(CHAT_MODEL, messages)
    audio_url = speak(assistant, voice_id) if assistant else ""
    return {"text": assistant, "audio_url": audio_url, "voice": voice_id}

# ---------------- Vision ----------------

def _vision_mode_hint(mode: str, target: str) -> str:
    m = (mode or "scene").strip().lower()
    if m == "emotion":
        return "Describe the likely facial expression and overall emotion in the scene. Be careful and tentative."
    if m == "navigate":
        t = target or "the target"
        return f"Help a visually impaired user navigate toward {t}. Mention obstacles and safe directions succinctly."
    if m == "objects":
        return "List the main objects you can see with very short positions (e.g., 'mug - bottom right')."
    # default: scene
    return "Describe the scene briefly for navigation; mention obstacles, walkable directions, and affordances."

def _pick_text_prompt(user_prompt: str, mode: str, target: str) -> str:
    user_prompt = (user_prompt or "").strip()
    hint = _vision_mode_hint(mode, target)
    return (user_prompt + "\n\n" if user_prompt else "") + hint

@api.post("/vision")
async def vision_endpoint(
    request: Request,
    authorization: Optional[str] = Header(None),
    # multipart (GUI path)
    image: UploadFile | None = File(None),
    prompt: str | None = Form(None),
    mode: str | None = Form(None),
    target: str | None = Form(None),
    voice_id: str | None = Form(None),
    # raw JSON (programmatic)
    payload: Dict[str, Any] | None = Body(None),
):
    _require_key(authorization)
    import base64

    # Pull inputs from either multipart or JSON
    text_prompt = (prompt or (payload or {}).get("prompt") or "").strip()
    v_id = (voice_id or (payload or {}).get("voice_id") or "en_US-amy-medium")
    mode = (mode or (payload or {}).get("mode") or "scene").lower()
    target = (target or (payload or {}).get("target") or "").strip()

    # NEW: max_words support
    max_words = None
    incoming_mw = (payload or {}).get("max_words") if payload else None
    try:
        max_words = int(incoming_mw) if incoming_mw is not None else None
    except Exception:
        max_words = None
    # Also accept multipart form field "max_words"
    if max_words is None:
        try:
            # starlette parses form fields as str
            form_mw = await request.form() if image is not None else None
            if form_mw and "max_words" in form_mw:
                max_words = int(str(form_mw["max_words"]))
        except Exception:
            max_words = None

    # Read image -> base64
    img_b64 = None
    if image is not None:
        data = await image.read()
        img_b64 = base64.b64encode(data).decode("ascii")
    elif payload and payload.get("image_b64"):
        img_b64 = str(payload["image_b64"])

    if not img_b64:
        raise HTTPException(status_code=400, detail="image missing (multipart 'image' or JSON 'image_b64')")

    # Build instruction hint based on mode
    if mode == "scene":
        sys_hint = "Describe the scene briefly for navigation."
    elif mode == "emotion":
        sys_hint = "Describe the likely facial expression and overall emotion."
    elif mode == "navigate":
        sys_hint = f"How would you navigate toward {target or 'the target'}? Mention obstacles and directions."
    elif mode == "objects":
        sys_hint = "List the main objects and their approximate positions (e.g., 'mug - bottom-right'). Keep it short."
    elif mode == "qa":  # NEW: free-form VQA
        # If user didn't give a question, make it explicit
        sys_hint = "Answer the user's question about the image as precisely as possible."
        if not text_prompt:
            text_prompt = "Answer the user's question about this image."
    else:
        sys_hint = "Describe the scene briefly."

    # Apply optional word limit
    if max_words and max_words > 0:
        sys_hint = f"{sys_hint} Use at most {max_words} words."

    # Final question sent to the model
    q = (text_prompt + "\n\n" if text_prompt else "") + sys_hint

    try:
        # Uses /api/chat with images=[b64] inside ollama_generate
        result = await ollama_generate(VISION_MODEL, q, image_b64=img_b64)
        # Simple fallback: if model returns empty, try a secondary model if configured
        if not result:
            fb = os.getenv("FALLBACK_VISION_MODEL", "").strip()
            if fb:
                result = await ollama_generate(fb, q, image_b64=img_b64)
    except Exception as e:
        raise HTTPException(status_code=502, detail=f"vision upstream error: {e}")

    audio_url = ""
    try:
        if result:
            audio_url = speak(result, v_id)
    except Exception:
        audio_url = ""

    return {"text": result or "", "audio_url": audio_url, "voice": v_id}


async def ollama_chat(model: str, messages: List[Dict[str, str]]) -> str:
    """
    Minimal, robust text chat using Ollama /api/generate (text-only).
    We just stitch user/system into one prompt to avoid any schema mismatch.
    """
    # Build a single plain prompt
    parts = []
    for m in messages:
        role = m.get("role","user")
        content = m.get("content","")
        if role == "system":
            parts.append(f"[SYSTEM]\n{content}\n")
        elif role == "assistant":
            parts.append(f"[ASSISTANT]\n{content}\n")
        else:
            parts.append(f"[USER]\n{content}\n")
    prompt = "\n".join(parts).strip()

    async with httpx.AsyncClient(timeout=120.0) as c:
        r = await c.post(f"{OLLAMA_URL}/api/generate", json={
            "model": model,
            "prompt": prompt,
            "stream": False
        })
        r.raise_for_status()
        j = r.json()
        return j.get("response","")

@api.post("/chat_diag")
async def chat_diag(payload: Dict[str, Any], authorization: Optional[str] = Header(None)):
    _require_key(authorization)
    text = str(payload.get("text","")).strip()
    if not text:
        raise HTTPException(status_code=400, detail="Empty text")
    parts = [
        {"role":"system","content":"You are a concise helpful assistant."},
        {"role":"user","content":text}
    ]
    prompt = "\n".join(f"[{m['role'].upper()}]\n{m['content']}" for m in parts)
    try:
        async with httpx.AsyncClient(timeout=120.0) as c:
            r = await c.post(f"{OLLAMA_URL}/api/generate", json={
                "model": CHAT_MODEL,
                "prompt": prompt,
                "stream": False
            })
            code = r.status_code
            body = r.text
            j = {}
            try:
                j = r.json()
            except Exception:
                pass
    except Exception as e:
        return {"diag": {"error": str(e)}}
    return {"diag": {"status": code, "first_400": body[:400]}, "parsed": j.get("response","")}
