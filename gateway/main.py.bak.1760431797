import os, uuid, glob, base64, shlex, subprocess, io, json
from typing import Optional, List, Set, Dict, Any
from fastapi import FastAPI, Header, HTTPException, UploadFile, File, Form, Body, Form, Body, Request, Request, Form, Body, Form, Body, Request
from fastapi.responses import FileResponse, JSONResponse
import httpx
import os
from pathlib import Path
import logging

# Base directory for writable data (audio/files/cache)
BASE_DIR = os.environ.get("TALKBOT_DATA_DIR", str(Path.cwd() / "runtime"))
AUDIO_DIR = str(Path(BASE_DIR) / "audio")
UPLOAD_DIR = str(Path(BASE_DIR) / "uploads")
CACHE_DIR = str(Path(BASE_DIR) / "cache")
os.makedirs(AUDIO_DIR, exist_ok=True)
os.makedirs(UPLOAD_DIR, exist_ok=True)
os.makedirs(CACHE_DIR, exist_ok=True)

AUDIO_DIR   = os.getenv("AUDIO_DIR", AUDIO_DIR)
API_KEYS_FN = os.getenv("API_KEYS_FILE", "/app/api_keys.txt")
VOICES_DIR  = "/data"  # where .onnx models live
OLLAMA_URL  = os.getenv("OLLAMA_URL", "http://ollama:11434")
CHAT_MODEL  = os.getenv("CHAT_MODEL",  "mistral:latest")
VISION_MODEL= os.getenv("VISION_MODEL","moondream:latest")

os.makedirs(AUDIO_DIR, exist_ok=True)

# -------- auth helpers --------
def _load_keys() -> Set[str]:
    keys: Set[str] = set()
    try:
        with open(API_KEYS_FN, "r", encoding="utf-8") as f:
            for ln in f:
                ln = ln.strip()
                if ln:
                    keys.add(ln)
    except FileNotFoundError:
        pass
    return keys

def _require_key(authorization: Optional[str]) -> None:
    if not authorization or not authorization.startswith("Bearer "):
        raise HTTPException(status_code=403, detail="Invalid API key")
    token = authorization.split(" ", 1)[1].strip()
    keys = _load_keys()
    if token not in keys:
        raise HTTPException(status_code=403, detail=f"Invalid API key (loaded={len(keys)} from {API_KEYS_FN})")

# -------- voice helpers --------
def _voice_files() -> Dict[str, str]:
    out: Dict[str, str] = {}
    for p in glob.glob(os.path.join(VOICES_DIR, "*.onnx")):
        vid = os.path.splitext(os.path.basename(p))[0]
        out[vid] = p
    return out

def _pretty_name(voice_id: str) -> str:
    # en_US-amy-medium -> en US / amy / medium
    return voice_id.replace("_", " ").replace("-", " / ")

# -------- TTS --------
def synth_with_piper(text: str, voice_id: str) -> str:
    models = _voice_files()
    if voice_id not in models:
        raise RuntimeError(f"Voice model not found for Piper: {voice_id}")
    out_id  = f"{uuid.uuid4()}.wav"
    out_fp  = os.path.join(AUDIO_DIR, out_id)
    modelfp = models[voice_id]
    # Piper static binary is on PATH (Dockerfile install)
    cmd = f'piper -m {shlex.quote(modelfp)} -f {shlex.quote(out_fp)}'
    proc = subprocess.Popen(shlex.split(cmd), stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    stdout, stderr = proc.communicate(text.encode("utf-8"), timeout=120)
    if proc.returncode != 0:
        raise RuntimeError(f"Piper failed: {stderr.decode('utf-8', errors='ignore')}")
    return f"/audio/{out_id}"

def synth_with_espeak(text: str, lang: str = "en-US", rate: int = 165) -> str:
    out_id = f"{uuid.uuid4()}.wav"
    out_fp = os.path.join(AUDIO_DIR, out_id)
    cmd = f'espeak-ng -v {shlex.quote(lang)} -s {rate} -w {shlex.quote(out_fp)} -- {shlex.quote(text)}'
    subprocess.run(cmd, shell=True, check=True)
    return f"/audio/{out_id}"

def speak(text: str, voice_id: str) -> str:
    try:
        # prefer Piper if model exists
        if voice_id in _voice_files():
            return synth_with_piper(text, voice_id)
        # else try espeak with best-effort language from voice_id
        lang = "en-US"
        if voice_id.startswith("en_GB"): lang = "en-GB"
        if voice_id.startswith("en_AU"): lang = "en-AU"
        return synth_with_espeak(text, lang=lang)
    except Exception as e:
        # final fallback (no TTS available)
        return ""

# -------- Ollama helpers --------
async def ollama_chat(model: str, messages: List[Dict[str, str]]) -> str:
    async with httpx.AsyncClient(timeout=120.0) as c:
        r = await c.post(f"{OLLAMA_URL}/api/generate", json={"model": model, "messages": messages, "stream": False})
        r.raise_for_status()
        j = r.json()
        # v3
        if "message" in j and isinstance(j["message"], dict):
            return j["message"].get("content", "")
        # v2-ish
        if "choices" in j and j["choices"]:
            return j["choices"][0]["message"]["content"]
        # fallback
        return j.get("response", "")
async def ollama_generate(model: str, prompt: str, image_b64: Optional[str] = None) -> str:
    """
    Use Ollama /api/generate for both text and vision.
    - Vision: payload includes "images": [<b64>]
    - Text:   payload is just prompt
    """
    async with httpx.AsyncClient(timeout=120.0) as c:
        if image_b64:
            payload = {
                "model": model,
                "prompt": prompt,
                "images": [image_b64],
                "stream": False,
            }
            r = await c.post(f"{OLLAMA_URL}/api/generate", json=payload)
            r.raise_for_status()
            j = r.json()
            return (j.get("response") or "").strip()

        payload = {"model": model, "prompt": prompt, "stream": False}
        r = await c.post(f"{OLLAMA_URL}/api/generate", json=payload)
        r.raise_for_status()
        j = r.json()
        return (j.get("response") or "").strip()

        # Text-only path
        payload = {"model": model, "prompt": prompt, "stream": False}
        r = await c.post(f"{OLLAMA_URL}/api/generate", json=payload)
        r.raise_for_status()
        j = r.json()
        return (j.get("response") or "").strip()

# -------- FastAPI --------
api = FastAPI()
log = logging.getLogger("talkbot.gateway")

@api.get("/health")
def health() -> Dict[str, bool]:
    return {"ok": True}

@api.get("/audio/{name}")
def get_audio(name: str):
    fp = os.path.join(AUDIO_DIR, name)
    if not os.path.isfile(fp):
        raise HTTPException(status_code=404, detail="File not found")
    return FileResponse(fp, media_type="audio/wav")

@api.get("/voices")
def voices(authorization: Optional[str] = Header(None)):
    _require_key(authorization)
    vids = sorted(_voice_files().keys())
    # if you want to expose only friendly names to the GUI:
    return {"voices": [{"id": vid, "name": _pretty_name(vid)} for vid in vids]}

@api.post("/speak")
def tts_endpoint(payload: Dict[str, Any], authorization: Optional[str] = Header(None)):
    _require_key(authorization)
    text = str(payload.get("text", "")).strip()
    voice_id = str(payload.get("voice_id", "en_US-amy-medium"))
    if not text:
        raise HTTPException(status_code=400, detail="Empty text")
    url = speak(text, voice_id)
    return {"audio_url": url, "voice": voice_id}

@api.post("/chat")
async def chat_endpoint(payload: Dict[str, Any], authorization: Optional[str] = Header(None)):
    _require_key(authorization)
    text = str(payload.get("text", "")).strip()
    voice_id = str(payload.get("voice_id", "en_US-amy-medium"))
    if not text:
        raise HTTPException(status_code=400, detail="Empty text")
    # Mistral chat
    sys_prompt = "You are a concise helpful assistant. Answer clearly."
    assistant = await ollama_chat(CHAT_MODEL, [{"role":"system","content":sys_prompt},
                                              {"role":"user","content":text}])
    audio_url = speak(assistant, voice_id)
    return {"text": assistant, "audio_url": audio_url, "voice": voice_id}



@api.post("/vision")
async def vision_endpoint(
    authorization: Optional[str] = Header(None),
    # multipart (GUI path)
    image: UploadFile | None = File(None),
    prompt: str | None = Form(None),
    mode: str | None = Form(None),        # new
    target: str | None = Form(None),      # new
    voice_id: str | None = Form(None),
    # raw JSON (programmatic)
    payload: Dict[str, Any] | None = Body(None),
):
    _require_key(authorization)
    import base64

    # gather inputs from multipart OR json
    text_prompt = (prompt or (payload or {}).get("prompt") or "").strip()
    v_id        = (voice_id or (payload or {}).get("voice_id") or "en_US-amy-medium")
    mode        = (mode or (payload or {}).get("mode") or "scene").strip().lower()
    target      = (target or (payload or {}).get("target") or "").strip()

    img_b64 = None
    if image is not None:
        data = await image.read()
        img_b64 = base64.b64encode(data).decode("ascii")
    elif payload and payload.get("image_b64"):
        img_b64 = str(payload["image_b64"])

    if not img_b64:
        raise HTTPException(status_code=400, detail="image missing (multipart 'image' or JSON 'image_b64')")

    TASKS = {
        "scene": (
            "Describe the scene briefly. Mention key objects, layout, depth cues, and potential obstacles. "
            "Be specific but concise."
        ),
        "emotion": (
            "Describe the apparent emotion or affect of visible people (if any). "
            "Comment on facial expression, posture, and overall mood. "
            "If faces are unclear, say so."
        ),
        "navigate": (
            "Given a target described by the user, outline step-by-step navigation from the camera's point of view "
            "to reach it safely. Call out obstacles, directions (left/right/ahead), and approximate distances. "
            f"Target: {target or 'not specified'}."
        ),
        "objects": (
            "List notable objects you can identify and their rough positions relative to the camera "
            "(e.g., 'mug — lower right, near'; 'door — center, far'). Be concise."
        ),
    }
    task_hint = TASKS.get(mode, TASKS["scene"])

    q = (text_prompt + "\n\n" if text_prompt else "") + task_hint

    try:
        result = await ollama_generate(VISION_MODEL, q, image_b64=img_b64)
    except Exception as e:
        raise HTTPException(status_code=502, detail=f"vision upstream error: {e}")

    # VM has no espeak/piper; GUI will do local TTS if enabled
    return {"text": result or "", "audio_url": "", "voice": v_id}

async def vision_endpoint(request: Request, authorization: Optional[str] = Header(None)):
    _require_key(authorization)
    import base64

    # Defaults
    text_prompt = ""
    voice_id = "en_US-amy-medium"
    img_b64 = None

    ctype = request.headers.get("content-type", "").lower()
    if ctype.startswith("application/json"):
        # JSON body: {"prompt": "...", "image_b64": "...", "voice_id": "..."}
        payload = await request.json()
        text_prompt = str((payload or {}).get("prompt", "")).strip()
        voice_id = str((payload or {}).get("voice_id", voice_id))
        img_b64 = (payload or {}).get("image_b64")
    else:
        # multipart/form-data
        form = await request.form()
        if "image" in form and hasattr(form["image"], "read"):
            data = await form["image"].read()
            img_b64 = base64.b64encode(data).decode("ascii")
        text_prompt = str(form.get("prompt", "") or "").strip()
        voice_id = str(form.get("voice_id", voice_id))

    if not img_b64:
        raise HTTPException(status_code=400, detail="image missing (multipart 'image' or JSON 'image_b64')")

        # Keep it simple: Moondream returns empty sometimes when over-prompted
    q = text_prompt if text_prompt else "Describe the scene for navigation."

    try:
        result = await ollama_generate(VISION_MODEL, q, image_b64=img_b64)
    except Exception as e:
        raise HTTPException(status_code=502, detail=f"vision upstream error: {e}")

    # No server-side TTS on the VM; return text (frontend can speak locally)
    audio_url = ""
    try:
        if result:
            audio_url = speak(result, voice_id)
    except Exception:
        audio_url = ""

    return {"text": result or "", "audio_url": audio_url, "voice": voice_id}

@api.get("/_authdebug")
def authdebug(authorization: Optional[str] = Header(None)):
    tok = ""
    if authorization and authorization.startswith("Bearer "):
        tok = authorization.split(" ", 1)[1].strip()
    return {
        "authorization": authorization,
        "token_len": len(tok),
        "token_prefix": tok[:8],
        "token_suffix": tok[-8:],
        "API_KEYS_FN": API_KEYS_FN,
        "loaded_keys": len(_load_keys()),
    }


@api.post("/vision_diag")
async def vision_diag(request: Request, authorization: Optional[str] = Header(None)):
    _require_key(authorization)
    import base64, httpx

    info = {"parsed": {}, "ollama_raw": None, "error": None}

    # Parse both JSON and multipart like /vision
    ctype = (request.headers.get("content-type") or "").lower()
    try:
        if ctype.startswith("application/json"):
            payload = await request.json()
            text_prompt = str((payload or {}).get("prompt", "")).strip()
            img_b64 = (payload or {}).get("image_b64")
            voice_id = str((payload or {}).get("voice_id", "en_US-amy-medium"))
        else:
            form = await request.form()
            text_prompt = str(form.get("prompt", "") or "").strip()
            voice_id = str(form.get("voice_id", "en_US-amy-medium"))
            img_b64 = None
            if "image" in form and hasattr(form["image"], "read"):
                data = await form["image"].read()
                img_b64 = base64.b64encode(data).decode("ascii")

        info["parsed"] = {
            "has_image_b64": bool(img_b64),
            "image_b64_prefix": (img_b64 or "")[:32],
            "prompt_len": len(text_prompt),
            "voice_id": voice_id,
        }

        if not img_b64:
            return {"diag": info, "note": "no image_b64 found; send multipart with image or JSON with image_b64"}

        # Build the same prompt used in /vision
        nav_hint = ("You are a navigation assistant. Describe the scene briefly and call out obstacles, "
                    "walkable directions, and any affordances. Be specific but concise.")
        q = (text_prompt + "\n\n" if text_prompt else "") + nav_hint

        # Call Ollama /api/generate directly
        async with httpx.AsyncClient(timeout=120.0) as c:
            payload = {
                "model": VISION_MODEL,
                "messages": [{"role": "user", "content": q, "images": [img_b64]}],
                "stream": False
            }
            r = await c.post(f"{OLLAMA_URL}/api/generate", json=payload)
            info["ollama_status"] = r.status_code
            info["ollama_text_first_400"] = r.text[:400]
            try:
                info["ollama_raw"] = r.json()
            except Exception as je:
                info["error"] = f"JSON parse error from Ollama: {je}"

        # Extract assistant content if present
        content = ""
        j = info.get("ollama_raw")
        if isinstance(j, dict) and "message" in j and isinstance(j["message"], dict):
            content = j["message"].get("content", "") or ""
        elif isinstance(j, dict) and "response" in j:
            content = j.get("response", "") or ""

        return {"diag": info, "extracted_content": content}

    except Exception as e:
        info["error"] = f"vision_diag exception: {e}"
        return {"diag": info}

            # ---- Attempt B: /api/generate ----
            try:
                payload = {
                    "model": model,
                    "prompt": prompt,
                    "images": [image_b64],
                    "stream": False
                }
                r = await c.post(f"{OLLAMA_URL}/api/generate", json=payload)
                r.raise_for_status()
                j = r.json()
                content = (j.get("response") or "").strip()
                return content
            except Exception:
                return ""

        # Text-only path (no image)
        payload = {"model": model, "prompt": prompt, "stream": False}
        r = await c.post(f"{OLLAMA_URL}/api/generate", json=payload)
        r.raise_for_status()
        j = r.json()
        return (j.get("response") or "").strip()

        payload = {"model": model, "prompt": prompt, "stream": False}
        r = await c.post(f"{OLLAMA_URL}/api/generate", json=payload)
        r.raise_for_status()
        j = r.json()
        return (j.get("response") or "").strip()

        payload = {"model": model, "prompt": prompt, "stream": False}
        r = await c.post(f"{OLLAMA_URL}/api/generate", json=payload)
        r.raise_for_status()
        j = r.json()
        return (j.get("response") or "").strip()
